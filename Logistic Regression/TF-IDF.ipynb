{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression: TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "A description of an execution of linear regression / logistic regression\n",
        "\n",
        "TF-IDF weighs words based on their frequency in a document and their rarity across all documents. This helps the model prioritize important, unique words, improving phishing detection accuracy."
      ],
      "metadata": {
        "id": "YdFYuCQQ918w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVFOOGp49vj5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=150000)\n",
        "X_tfidf = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Step 2: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Step 3: Define Logistic Regression Model in PyTorch\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)  # Linear layer with one output for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()            # Apply sigmoid for probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))\n",
        "\n",
        "# Initialize Model\n",
        "input_size = X_train_tensor.shape[1]\n",
        "model = LogisticRegressionModel(input_size)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 4: Train the Model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    permutation = torch.randperm(X_train_tensor.size(0))\n",
        "\n",
        "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward Pass and Optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = model(X_test_tensor)\n",
        "    y_pred = (y_pred_probs > 0.5).int()\n",
        "\n",
        "# Print Metrics\n",
        "accuracy = accuracy_score(y_test_tensor, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_tensor, y_pred, target_names=[\"Legitimate\", \"Phishing\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "DataFrame Head:\n",
        "                                                text  label\n",
        "0  re : 6 . 1100 , disc : uniformitarianism , re ...      0\n",
        "1  the other side of * galicismos * * galicismo *...      0\n",
        "2  re : equistar deal tickets are you still avail...      0\n",
        "3  \\nHello I am your hot lil horny toy.\\n    I am...      1\n",
        "4  software at incredibly low prices ( 86 % lower...      1\n",
        "\n",
        "Accuracy: 0.9553\n",
        "\n",
        "Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "  Legitimate       0.94      0.99      0.96      2493\n",
        "    Phishing       0.98      0.90      0.94      1535\n",
        "\n",
        "    accuracy                           0.96      4028\n",
        "   macro avg       0.96      0.95      0.95      4028\n",
        "weighted avg       0.96      0.96      0.95      4028\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "q-fD4Hrz-YuZ"
      }
    }
  ]
}