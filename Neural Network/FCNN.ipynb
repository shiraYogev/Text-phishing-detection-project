{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FCNN**\n",
        "\n",
        "The Fully Connected Neural Network is more advanced Neural Network.The text data is converted into numeric features, and a PyTorch-based model is trained to make predictions. The FCNN contain 2 layers and one of them is hidden layer,\n",
        "Also The FCNN uses activation function such as ReLU.\n",
        "The downside of FCNN is that the computation cost of FCNN is higher than the SC.\n",
        "\n"
      ],
      "metadata": {
        "id": "YdFYuCQQ918w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVFOOGp49vj5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "# Load the phishing dataset\n",
        "dataset = load_dataset(\"ealvaradob/phishing-dataset\", \"texts\", trust_remote_code=True)\n",
        "# Check the structure of the dataset\n",
        "print(\"Dataset Head:\")\n",
        "print(dataset['train'][:5])  # Print the first 5 entries of the dataset\n",
        "\n",
        "# Extract text and labels\n",
        "texts = [example['text'] for example in dataset['train']]\n",
        "labels = [example['label'] for example in dataset['train']]\n",
        "\n",
        "# Convert to DataFrame for better visualization and sanity check\n",
        "df = pd.DataFrame({'text': texts, 'label': labels})\n",
        "print(\"\\nDataFrame Head:\")\n",
        "print(df.head())\n",
        "# Step 1: TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=150000)\n",
        "X_tfidf = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Step 2: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Step 3: Define Fully Connected Neural Network\n",
        "class SimpleFCNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleFCNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer 1\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)          # Fully connected layer 2 (output)\n",
        "        self.sigmoid = nn.Sigmoid()                   # Apply sigmoid for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation after fc1\n",
        "        x = self.sigmoid(self.fc2(x))  # Apply sigmoid after fc2\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "input_size = X_train_tensor.shape[1]\n",
        "hidden_size = 128  # Number of neurons in the hidden layer\n",
        "model = SimpleFCNN(input_size, hidden_size)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 4: Train the Model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    permutation = torch.randperm(X_train_tensor.size(0))\n",
        "\n",
        "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward Pass and Optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = model(X_test_tensor)\n",
        "    y_pred = (y_pred_probs > 0.5).int()\n",
        "\n",
        "# Print Metrics\n",
        "accuracy = accuracy_score(y_test_tensor, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_tensor, y_pred, target_names=[\"Legitimate\", \"Phishing\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "DataFrame Head:\n",
        "                                                text  label\n",
        "0  re : 6 . 1100 , disc : uniformitarianism , re ...      0\n",
        "1  the other side of * galicismos * * galicismo *...      0\n",
        "2  re : equistar deal tickets are you still avail...      0\n",
        "3  \\nHello I am your hot lil horny toy.\\n    I am...      1\n",
        "4  software at incredibly low prices ( 86 % lower...      1\n",
        "\n",
        "Epoch [1/10], Loss: 0.1542\n",
        "Epoch [2/10], Loss: 0.0516\n",
        "Epoch [3/10], Loss: 0.0044\n",
        "Epoch [4/10], Loss: 0.0032\n",
        "Epoch [5/10], Loss: 0.0022\n",
        "Epoch [6/10], Loss: 0.0041\n",
        "Epoch [7/10], Loss: 0.0010\n",
        "Epoch [8/10], Loss: 0.0010\n",
        "Epoch [9/10], Loss: 0.0006\n",
        "Epoch [10/10], Loss: 0.0011\n",
        "\n",
        "Accuracy: 0.9767\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "  Legitimate       0.98      0.99      0.98      2493\n",
        "    Phishing       0.98      0.96      0.97      1535\n",
        "\n",
        "    accuracy                           0.98      4028\n",
        "   macro avg       0.98      0.97      0.98      4028\n",
        "weighted avg       0.98      0.98      0.98      4028\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "q-fD4Hrz-YuZ"
      }
    }
  ]
}